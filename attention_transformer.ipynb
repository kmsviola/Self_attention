{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/ndb796/Deep-Learning-Paper-Review-and-Practice/blob/master/code_practices/Attention_is_All_You_Need_Tutorial_(German_English).ipynb\n",
    "\n",
    "Attention is All You Need (NIPS 2017) 실습\n",
    "본 코드는 기본적으로 Transformer 논문의 내용을 최대한 따릅니다.\n",
    "본 논문은 딥러닝 기반의 자연어 처리 기법의 기본적인 구성을 이해하고 공부하는 데에 도움을 줍니다.\n",
    "2020년 기준 가장 뛰어난 번역 모델들은 본 논문에서 제안한 Transformer 기반의 아키텍처를 따르고 있습니다.\n",
    "코드 실행 전에 [런타임] → [런타임 유형 변경] → 유형을 GPU로 설정합니다.\n",
    "BLEU Score 계산을 위한 라이브러리 업데이트\n",
    "[Restart Runtime] 버튼을 눌러 런타임을 재시작할 필요가 있습니다.\n",
    "\n",
    "설치하기\n",
    "!pip install torchtext==0.6.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('nlp_37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "651f2c05a74e05f366f30fe568aef15b16e18e43a48cebba056826fa67245533"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
